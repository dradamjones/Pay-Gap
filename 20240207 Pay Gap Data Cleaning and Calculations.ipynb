{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d37ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5a171-33c0-4956-a71d-45fc8ac4877a",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load payroll data\n",
    "# If these reports are run in one combined report with a payroll area identifier column there's no need to clean and concat them\n",
    "\n",
    "m1_wage_df = pd.read_excel(wage_path_m1)  # list of all wage types with amounts paid for the full year, UNN\n",
    "m2_wage_df = pd.read_excel(wage_path_m2) # list of all wage types with amounts paid for the full year, UNN  \n",
    "m1_sick_pay_df = pd.read_excel(sick_path_m1) # list of all sickness payments made for the pay period, NUSL\n",
    "m2_sick_pay_df = pd.read_excel(sick_path_m2) # list of all sickness payments made for the pay period, NUSL\n",
    "absence_df = pd.read_excel(absence_path) # all absence types during the pay period  \n",
    "leavers_df = pd.read_excel(leavers_path)  # all leavers during the pay period\n",
    "stafflist_df = pd.read_excel(stafflist_path) # all contracted staff as of the census date\n",
    "starters_df = pd.read_excel(starters_path) # starters as of the census date\n",
    "gem_df = pd.read_excel(gem_path) # recipients of gem awards and long service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7f3517-0c1f-45a7-8392-3f059c06f83a",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31743e-a74a-4da7-bb08-5cfe06f76071",
   "metadata": {},
   "source": [
    "### Create a mapping dictionary for recoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b3040-448b-4536-bd1e-d6398d3aa09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_mapping = {\n",
    "    '01': '1',\n",
    "    '02': '2',\n",
    "    '03': '3',\n",
    "    '04': '4',\n",
    "    '05': '5',\n",
    "    '06': '6',\n",
    "    '07': '7',\n",
    "    '08': '8',\n",
    "    '09': '9',\n",
    "    '10': '10',\n",
    "    '11': '11',\n",
    "    'CWD': 'MISC',\n",
    "    'CWF': 'MISC',\n",
    "    'CWB_N': 'MISC',\n",
    "    'CWG': 'MISC',\n",
    "    'APP': 'MISC',\n",
    "    'CWB': 'MISC',\n",
    "    'CWH': 'MISC',\n",
    "    'CWE': 'MISC',\n",
    "    'STP': 'MISC',\n",
    "    'CWC': 'MISC',\n",
    "    'DEM05': 'MISC',\n",
    "    'VL': 'MISC',\n",
    "    'INV': 'MISC',\n",
    "    'EXTE': 'MISC',\n",
    "    'VT': 'MISC',\n",
    "    'AA05': 'MISC'\n",
    "}\n",
    "\n",
    "# Recode the 'Grade' column using the mapping\n",
    "stafflist_df['Grade'] = stafflist_df['Grade'].map(grade_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405e0805-ad40-48af-a7e0-4d65f84a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gem_df = gem_df.dropna(subset=['StaffNo'])\n",
    "gem_df['StaffNo'].value_counts()\n",
    "\n",
    "# Convert 'StaffNo' to integer\n",
    "gem_df['StaffNo'] = gem_df['StaffNo'].astype(int)\n",
    "# Drop rows where 'StaffNo' is NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05933c1e-4d40-40bf-b20c-c81f062261bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row 0 from stafflist\n",
    "stafflist_df = stafflist_df.drop(0)\n",
    "# Convert 'Personnel No' and 'Staff Id' columns to integers\n",
    "stafflist_df['Personnel No'] = stafflist_df['Personnel No'].astype(int)\n",
    "stafflist_df['Staff Id'] = stafflist_df['Staff Id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd94d62-8bd3-42c3-94d5-302120b23071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove row 0 from absence\n",
    "absence_df = absence_df.drop(0)\n",
    "# Convert 'Assignment No' and 'Staff ID' columns to integers\n",
    "absence_df['Assignment No'] = absence_df['Assignment No'].astype(int)\n",
    "absence_df['Staff ID'] = absence_df['Staff ID'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bdd109-2a55-4403-9383-0b215bd72619",
   "metadata": {},
   "source": [
    "### Create a mapping dictionary for sick pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397d4a7-64d1-47fc-9154-90dd5bb744d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "column_mapping = dict(zip(m1_sick_pay_df.columns, m2_sick_pay_df.columns))\n",
    "\n",
    "# Check if the mapping is as expected\n",
    "print(\"\\nColumn Mapping:\")\n",
    "for k, v in column_mapping.items():\n",
    "    print(f\"{k} will be renamed to {v}\")\n",
    "\n",
    "# Rename columns in m1_wage_df using the mapping\n",
    "m1_sick_pay_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add 'Payroll_Area' column with value 'M1' to m1_wage_df\n",
    "m1_sick_pay_df['Payroll_Area'] = 'M1'\n",
    "\n",
    "# Add 'Payroll_Area' column with value 'M2' to m2_wage_df\n",
    "m1_sick_pay_df['Payroll_Area'] = 'M2'\n",
    "\n",
    "# Concat M1 and M2\n",
    "sick_pay_df = pd.concat([m1_sick_pay_df, m2_sick_pay_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edaaa426-a3e9-44d8-bbad-20431136b626",
   "metadata": {},
   "source": [
    "### Create a mapping dictionary for wage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6c50c-42d1-4ab7-9c15-03781c874754",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = dict(zip(m1_wage_df.columns, m2_wage_df.columns))\n",
    "\n",
    "# Check if the mapping is as expected\n",
    "print(\"\\nColumn Mapping:\")\n",
    "for k, v in column_mapping.items():\n",
    "    print(f\"{k} will be renamed to {v}\")\n",
    "\n",
    "# Rename columns in m1_wage_df using the mapping\n",
    "m1_wage_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "# Add 'Payroll_Area' column with value 'M1' to m1_wage_df\n",
    "m1_wage_df['Payroll_Area'] = 'M1'\n",
    "\n",
    "# Add 'Payroll_Area' column with value 'M2' to m2_wage_df\n",
    "m2_wage_df['Payroll_Area'] = 'M2'\n",
    "\n",
    "# Concat M1 and M2\n",
    "mergewage_df = pd.concat([m1_wage_df, m2_wage_df], ignore_index=True)\n",
    "\n",
    "# Drop total gross\n",
    "mergewage_df = mergewage_df[mergewage_df['Wage_Text'] != 'Total gross']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e62e3-592b-4265-b626-9c9322d3b2d0",
   "metadata": {},
   "source": [
    "### add GEM awards to bonuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b64375-3be0-418e-9290-61b456fdcc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform 'StaffNo' in gem_df for 'Personnel_No'\n",
    "gem_df['Personnel_No'] = gem_df['StaffNo'].astype(str) + '000'\n",
    "gem_df['Personnel_No'] = gem_df['Personnel_No'].astype(int)\n",
    "# Rename columns in gem_df to match wage_df\n",
    "gem_df = gem_df.rename(columns={'StaffNo': 'Staff_Id',\n",
    "                                'Wage_Text': 'Wage_Text',\n",
    "                                'Amount': 'Amount',\n",
    "                                'Date': 'Payment_Date'})\n",
    "\n",
    "# Selecting only the relevant columns\n",
    "relevant_columns = ['Staff_Id', 'Personnel_No', 'Wage_Text', 'Amount', 'Payment_Date']\n",
    "gem_df_transformed = gem_df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c038c3-ada2-4257-b9a9-a1b870f5d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mergewage_df['Wage_Text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f47e2b-6735-4f05-b4ac-2a2edba4957d",
   "metadata": {},
   "source": [
    "### Prepare for merging staff lists and payroll data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead49881-a7c5-4593-9b0b-9855ec369589",
   "metadata": {},
   "outputs": [],
   "source": [
    "stafflist_df.columns = stafflist_df.columns.str.replace(\" \", \"_\").str.replace(\"/\", \"_\")\n",
    "overlap_columns = stafflist_df.columns.intersection(mergewage_df.columns).tolist()\n",
    "stafflist_df = stafflist_df.rename(columns={col: col + \"_SL\" for col in overlap_columns if col != 'Personnel_No'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea0bfd1-3b42-4be8-a40a-f2a2914ab69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmergewage_df = pd.concat([mergewage_df, gem_df_transformed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873dc0b8-d30a-455d-8113-5e733991ece6",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmergewage_df['Wage_Text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456321bd-f019-4f8d-adf2-203eb22373a4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "note we merge to the stafflist and not the other way round, so we can eliminate casual payments in wage_df \n",
    "to workers who aren't contracted (i.e. are not present in stafflist_df)</br></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6071c718-a3e7-461f-8928-22394d011ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "newmergewage_df['Personnel_No'] = newmergewage_df['Personnel_No'].astype(int)\n",
    "wage_df = pd.merge(stafflist_df, newmergewage_df, how='left', on=['Personnel_No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218586ac-3874-4e27-9a3f-f3fade66e540",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Drop any rows in wage_df that don't have a Payroll_Area_SL from stafflist_df, indicating not employees (regular or casual)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c5319d-b136-409a-914a-10bf61f01b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df = wage_df.dropna(subset=['Payroll_Area_SL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0c6f29-c680-4982-bee6-de57b247566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_amounts = wage_df.groupby('Wage_Text')['Amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919e01c-a426-4e55-9182-c85c377c126f",
   "metadata": {},
   "source": [
    "### Set payment year start and end dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf14f0e-8e73-42e2-83d7-fb22a4a4bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_date = pd.Timestamp('2022-04-01')\n",
    "end_date = pd.Timestamp('2023-03-31')\n",
    "\n",
    "# Filter the dataframe based on the date range\n",
    "wage_df = wage_df[\n",
    "    (wage_df['Payment_Date'] >= start_date) & \n",
    "    (wage_df['Payment_Date'] <= end_date)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b655e2-1bba-44ca-959f-9082abab4aec",
   "metadata": {},
   "source": [
    "__[gov.uk preparing your data](https://www.gov.uk/government/publications/gender-pay-gap-reporting-guidance-for-employers/preparing-your-data)__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29675dfa-598a-4d64-8495-0805b880fb6b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">For salaried staff we can use FTE</br></br>\n",
    "For employees who are not contracted to work the same number of hours each week, \n",
    "you need to work out a mean (average) for them.\n",
    "use the 12 weeks that ends with the last full week of the pay period that includes your snapshot date\n",
    "take the total number of hours worked by each employee over this period and divide it by 12\n",
    "If the 12-week period includes a week where the employee did not work at all, \n",
    "replace it with an earlier week where they worked.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea062e-dc19-4529-9030-ce1ae096304b",
   "metadata": {},
   "source": [
    "## Separate Relevant and Full-pay Relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80427bca-4c68-4f66-bfb6-63ff66215fab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Sickness exclusions</br></br>\n",
    "Note we can't filter Absence Year on 2022/23 in case of long term absences that may have started in the previous year</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d85b2f3-469d-4f97-9021-cdc22eddc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' in sick_pay_df to datetime\n",
    "sick_pay_df['Date'] = pd.to_datetime(sick_pay_df['Date'])\n",
    "\n",
    "# Filter for the specific date range\n",
    "start_date = '2023-03-01'\n",
    "end_date = '2023-03-31'\n",
    "sick_pay_df_filtered = sick_pay_df[(sick_pay_df['Date'] >= start_date) & (sick_pay_df['Date'] <= end_date)]\n",
    "\n",
    "# Identify individuals who received sick pay in the specified period\n",
    "sick_individuals = sick_pay_df_filtered['Personnel_No'].unique()\n",
    "\n",
    "# Flag these individuals in the wage data\n",
    "wage_df['Sickness_Exclusion'] = wage_df['Personnel_No'].isin(sick_individuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3c140f-6890-4f2e-9981-2a2bd87e1b27",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Absence exclusions. Strike days don't count even though unpaid</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505dc97e-1595-4c99-b6c4-51c9719ee307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Start Date' to datetime, coerce out-of-bounds dates to NaT\n",
    "absence_df['Start Date'] = pd.to_datetime(absence_df['Start Date'], errors='coerce')\n",
    "\n",
    "# Convert 'End Date' to datetime, coerce out-of-bounds dates to NaT\n",
    "absence_df['End Date'] = pd.to_datetime(absence_df['End Date'], errors='coerce')\n",
    "\n",
    "# Define the list of absence types to filter\n",
    "absence_types = [\n",
    "    'Career Break', 'Special Leave', 'Maternity leave 2022',\n",
    "    'Unpaid Leave', 'Shared Parental (Paid)', 'Shared Parental (Unpaid)', 'Paternity leave 2022'\n",
    "]\n",
    "# Filter absence_df and create absence_reduced_pay. Picks up anyone with reduced pay due to absence during the pay period\n",
    "absence_reduced_pay = absence_df[\n",
    "    (absence_df['Absence Type'].isin(absence_types)) &\n",
    "    (absence_df['Start Date'] <= pd.Timestamp('2023-03-31')) &\n",
    "    ((absence_df['End Date'] >= pd.Timestamp('2023-03-01')) | absence_df['End Date'].isna())\n",
    "]\n",
    "\n",
    "# Step 4: Create 'Absence_Exclusion' in wage_df\n",
    "# True if 'Personnel No' in wage_df is in 'Assignment No' in absence_reduced_pay\n",
    "wage_df['Absence_Exclusion'] = wage_df['Personnel_No'].isin(absence_reduced_pay['Assignment No'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0eb872-5da4-471d-98f0-70984b775a47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Starter exclusions - won't have been paid full salary for pay period</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132d58ac-520f-44ce-88c9-aef659606a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "starters_df['Latest Hire Date'] = pd.to_datetime(starters_df['Latest Hire Date'])\n",
    "\n",
    "# Define start and end date for the filter\n",
    "start_date = pd.Timestamp('2023-03-01')\n",
    "end_date = pd.Timestamp('2023-03-31')\n",
    "\n",
    "# Step 2: Filter starters_df based on the date range\n",
    "filtered_starters_df = starters_df[\n",
    "    (starters_df['Latest Hire Date'] >= start_date) & \n",
    "    (starters_df['Latest Hire Date'] <= end_date)\n",
    "]\n",
    "\n",
    "# Step 3: Create 'Starter_Exclusion' in wage_df\n",
    "# True if 'Personnel No' in wage_df is in 'Personnel_No' in filtered_starters_df\n",
    "wage_df['Starter_Exclusion'] = wage_df['Personnel_No'].isin(filtered_starters_df['Personnel No'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8f0dd-f90b-45d3-a8f0-80b3dc823997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check exclusion counts\n",
    "sickness_exclusion_counts = wage_df['Sickness_Exclusion'].value_counts()\n",
    "absence_exclusion_counts = wage_df['Absence_Exclusion'].value_counts()\n",
    "starter_exclusion_counts = wage_df['Starter_Exclusion'].value_counts()\n",
    "\n",
    "value_counts_table = pd.DataFrame({\n",
    "    'Sickness_Exclusion': sickness_exclusion_counts,\n",
    "    'Absence_Exclusion': absence_exclusion_counts,\n",
    "    'Starter_Exclusion': starter_exclusion_counts\n",
    "})\n",
    "\n",
    "value_counts_table = value_counts_table.reset_index()\n",
    "value_counts_table.columns = ['Value', 'Sickness_Exclusion', 'Absence_Exclusion', 'Starter_Exclusion']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb38cf98-0253-4184-8cb5-a08c6c26bd5f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Classify as relevant or full-pay relevant</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83eb92d-7ebd-4bd0-a772-8289b3c9b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "wage_df['Employee_Classification'] = np.where(\n",
    "    (wage_df['Sickness_Exclusion'] | wage_df['Absence_Exclusion'] | wage_df['Starter_Exclusion']),\n",
    "    'Relevant',\n",
    "    'Full-pay relevant'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf67bff-8add-47a8-a73e-98b2d50c1293",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df['Employee_Classification'].value_counts() # includes duplicate personnel nos as no filter yet on dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2255e79-8790-4a43-b2b3-c372d1dce7cf",
   "metadata": {},
   "source": [
    "## Classify wage type codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6034a-8a2b-4608-b9f4-9a48789d2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_sacrifice_types = [\n",
    "    'Cycle Scheme SS', 'Nursery SS', 'Holiday SS', \n",
    "    'Employee Pen Cont US5'\n",
    "]\n",
    "\n",
    "ordinary_pay_types = [\n",
    "    'Monthly Rotate.Shift All.', 'Monthly Altern.Shift All.', 'Salary', \n",
    "    'Protected Pay', 'Honorarium', 'Standby',\n",
    "    'Additional Payments', 'Childcare Voucher' # childcare voucher goes in here and not SS\n",
    "]\n",
    "\n",
    "ordinary_casual_types = [\n",
    "    'Casual Hrs -CWA-CWH', 'Casual Hours', 'Invigilator Supervisor', 'Invigilator',\n",
    "    'Variable Hours', 'Hourly @ 1', 'Academic Hourly',\n",
    "    'AL Marking'\n",
    "]\n",
    "\n",
    "bonus_pay_types = [\n",
    "    'Retention Supplement', 'Performance Pay', 'Bonus', 'Recruitment Supplement', 'GEM'\n",
    "]\n",
    "\n",
    "wage_df['Wage_Class'] = wage_df['Wage_Text'].apply(\n",
    "    lambda x: 'Bonus_Pay' if x in bonus_pay_types else (\n",
    "        'Ordinary_Casual' if x in ordinary_casual_types else (\n",
    "            'Ordinary_Pay' if x in ordinary_pay_types else (\n",
    "                'Salary_Sacrifice' if x in salary_sacrifice_types else 'Other'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490d8fd8-ced1-4609-b11b-a23a5731acdb",
   "metadata": {},
   "source": [
    "### Check unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a92815-fc2f-45a7-866d-de0ba0a4b1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_wage_text = set(wage_df['Wage_Text'].unique())\n",
    "\n",
    "# Union of sets of values in the lists\n",
    "all_wage_types = set(salary_sacrifice_types + ordinary_pay_types + ordinary_casual_types + bonus_pay_types)\n",
    "\n",
    "# Check if all unique values are represented in only one list\n",
    "if unique_wage_text == all_wage_types:\n",
    "    print(\"All unique values in wage_df['Wage_Text'] are represented in one and only one list.\")\n",
    "else:\n",
    "    print(\"Not all unique values in wage_df['Wage_Text'] are represented in one and only one list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3ed85-2c1c-4436-8c2c-e17383099f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are hours recorded in all ordinary_casual_types as we don't have FTE\n",
    "bonus_pay_df = wage_df[wage_df['Wage_Text'].isin(bonus_pay_types)]\n",
    "# Apply describe() to 'Number' column grouped by 'Wage_Text'\n",
    "describe_stats = bonus_pay_df.groupby('Wage_Text')['Amount'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64226b0a-091a-4ca4-ae18-862dc3ce2741",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Check FTEs for ordinary pay</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aea5bb-2b90-4c1c-b855-60356ee5bd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinary_pay_df = wage_df[wage_df['Wage_Text'].isin(ordinary_pay_types)]\n",
    "describe_stats = ordinary_pay_df.groupby('Wage_Text')['FTE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b7134-cfb8-4d66-af15-fdd1aa370989",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Check that there are hours recorded in all ordinary_casual_types as we don't have FTE</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf1342a-0470-4893-a356-7164898cbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinary_casual_df = wage_df[wage_df['Wage_Text'].isin(ordinary_casual_types)]\n",
    "# Apply describe() to 'Number' column grouped by 'Wage_Text'\n",
    "describe_stats = ordinary_casual_df.groupby('Wage_Text')['Number'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa519c-4b5e-40fe-a861-0acdfbbf78eb",
   "metadata": {},
   "source": [
    "## Reclassify bonus pay that should be ordinary pay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86429be-6381-471b-94eb-457ccbc3a555",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">for any bonus_pay_types that appear in the last three months for the same NINO, reclassify these as ordinary pay and not bonus\n",
    "</br>\n",
    "https://www.gov.uk/government/publications/gender-pay-gap-reporting-guidance-for-employers/preparing-your-data#step-2-add-ordinary-pay</br>\n",
    "\n",
    "You must include all allowances in ordinary pay. In some cases, you will need to decide whether a payment is an allowance or an expense.</br>\n",
    "\n",
    "You should include:</br>\n",
    "\n",
    "any allowances your employees would expect to receive in their regular pay</br>\n",
    "recruiting and retaining an employee – but treat ‘one off’ incentive payments as bonus pay, \n",
    "not an allowance, if you pay them at the start of employment or they are closer to a bonus than an ongoing allowance</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb336b-5d08-4e7f-a5b6-99221c511cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'Month_Year' to represent the month and year of 'Payment_Date'\n",
    "wage_df['Month_Year'] = wage_df['Payment_Date'].dt.to_period('M')\n",
    "\n",
    "# Create a mask for bonus payments in the specified period\n",
    "bonus_mask = (wage_df['Wage_Text'].isin(bonus_pay_types)) & (wage_df['Payment_Date'] >= '2022-04-01') & (wage_df['Payment_Date'] <= '2023-03-31')\n",
    "\n",
    "# Group by 'Personnel_No' and 'Wage_Text' and count unique 'Month_Year' values\n",
    "counts = wage_df[bonus_mask].groupby(['Personnel_No', 'Wage_Text'])['Month_Year'].nunique().reset_index()\n",
    "\n",
    "# Filter for 'Personnel_No' with three or more months of the same 'Wage_Text'\n",
    "qualified_personnel = counts[counts['Month_Year'] >= 3]\n",
    "\n",
    "# Update 'Wage_Class' for qualified personnel and 'bonus_pay_types'\n",
    "wage_df.loc[\n",
    "    (wage_df['Personnel_No'].isin(qualified_personnel['Personnel_No'])) &\n",
    "    (wage_df['Wage_Text'].isin(bonus_pay_types)),\n",
    "    'Wage_Class'\n",
    "] = 'Ordinary_Pay'\n",
    "\n",
    "# Remove the 'Month_Year' column if it's no longer needed\n",
    "wage_df.drop(columns=['Month_Year'], inplace=True)\n",
    "\n",
    "changed_wage_class = wage_df[(wage_df['Wage_Class'] == 'Ordinary_Pay') &\n",
    "                            (wage_df['Wage_Text'].isin(bonus_pay_types))]\n",
    "\n",
    "# Get unique 'Personnel_No' values\n",
    "changed_personnel_nos = changed_wage_class['Personnel_No'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cc4e94-1a65-4a5c-bfe2-7d82eddacd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df['Wage_Text'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961bd3e-0db6-41c7-b29c-56c0ffa8dc9f",
   "metadata": {},
   "source": [
    "### Count unique payment dates for each Personnel_No for bonus payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b24f52-9d67-4b0f-9d00-0e1b81ce05cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_payment_counts = wage_df[wage_df['Wage_Class'] == 'Bonus_Pay'].groupby('Personnel_No')['Payment_Date'].nunique()\n",
    "\n",
    "# Identify Personnel_No with more than one bonus payment date\n",
    "monthly_bonus_personnel = bonus_payment_counts[bonus_payment_counts > 1].index\n",
    "\n",
    "# Set 'Bonus_Period_Months' based on the count\n",
    "# Default to annual (12 months)\n",
    "wage_df['Bonus_Period_Months'] = 12\n",
    "\n",
    "# Set to monthly (1 month) for those identified with more than one payment date\n",
    "wage_df.loc[wage_df['Personnel_No'].isin(monthly_bonus_personnel) & (wage_df['Wage_Class'] == 'Bonus_Pay'), 'Bonus_Period_Months'] = 1\n",
    "\n",
    "# GEM period is 1 month as they can be awarded in multiple months\n",
    "wage_df['Bonus_Period_Months'] = np.where(wage_df['Wage_Text'] == 'GEM', 1, wage_df['Bonus_Period_Months'])\n",
    "\n",
    "# Constants\n",
    "DAYS_IN_MONTH = 30.44\n",
    "PAY_DATE = pd.Timestamp('2023-03-24')\n",
    "START_DATE = pd.Timestamp('2023-03-01')\n",
    "END_DATE = pd.Timestamp('2023-03-31')\n",
    "\n",
    "# Identify Bonus Payments in the Pay Period\n",
    "pay_period_bonus = wage_df[\n",
    "    (wage_df['Wage_Class'] == 'Bonus_Pay') &\n",
    "    (wage_df['Payment_Date'] >= START_DATE) &\n",
    "    (wage_df['Payment_Date'] <= END_DATE)\n",
    "]\n",
    "\n",
    "# Filter for relevant bonus payments around the snapshot date\n",
    "relevant_bonuses = wage_df[\n",
    "    (wage_df['Wage_Class'] == 'Bonus_Pay') &\n",
    "    (wage_df['Payment_Date'] == PAY_DATE) & \n",
    "    (~wage_df['Starter_Exclusion']) &  # Full-pay relevant employees (not excluded)\n",
    "    (~wage_df['Sickness_Exclusion']) &\n",
    "    (~wage_df['Absence_Exclusion'])\n",
    "]\n",
    "\n",
    "# After setting 'Bonus_Period_Months' as previously described, calculate the prorated bonus\n",
    "pay_period_bonus['Prorated_Bonus_Total'] = (\n",
    "    (pay_period_bonus['Amount'] / (pay_period_bonus['Bonus_Period_Months'] * DAYS_IN_MONTH)) * DAYS_IN_MONTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c69927-653b-44a5-ba7e-2a3a95747e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df = wage_df.merge(pay_period_bonus[['Personnel_No', 'Prorated_Bonus_Total']], on='Personnel_No', how='left')\n",
    "wage_df['Wage_Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a29358-156e-4929-ab85-e9c64df38836",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df = wage_df[wage_df['Wage_Class'] != 'Other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b717536-0db4-4a15-938d-4cbfa573dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df['Wage_Text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ce50de-7bc4-4da1-b7e1-8c0eff63ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get combinations of 'Wage_Class' and 'Wage_Text' and count occurrences\n",
    "combination_counts = wage_df.groupby(['Wage_Class', 'Wage_Text', 'Payment_Date']).size().reset_index(name='Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3b1d3-8c63-49a4-8a7c-2d6531e43536",
   "metadata": {},
   "source": [
    "##  Create table of bonuses with relevant and full-pay relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5180a1c4-f9c4-44f1-9821-ad0facff5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot for Total_Bonus_Pay and Total_Bonus_Count\n",
    "bonus_pay_df = wage_df[(wage_df['Wage_Class'] == 'Bonus_Pay') & \n",
    "                       (wage_df['Payment_Date'] >= '2022-04-01') & \n",
    "                       (wage_df['Payment_Date'] <= '2023-03-31')]\n",
    "pivot_bonus_pay = bonus_pay_df.pivot_table(index='Personnel_No', \n",
    "                                           values='Amount', \n",
    "                                           aggfunc={'Amount': 'sum', 'Personnel_No': 'count'}, \n",
    "                                           fill_value=0)\n",
    "pivot_bonus_pay.rename(columns={'Amount': 'Full_Total_Bonus', 'Personnel_No': 'Total_Bonus_Count'}, inplace=True)\n",
    "\n",
    "bonus_df = pd.merge(pivot_bonus_pay, stafflist_df, on='Personnel_No', how='left')\n",
    "bonus_df = bonus_df.sort_values(by='Full_Total_Bonus', ascending=False)\n",
    "bonus_df_unique_personnel_no = bonus_df.drop_duplicates(subset=['Personnel_No'], keep='first')\n",
    "bonus_df = bonus_df_unique_personnel_no.fillna(0)\n",
    "bonus_df['Wage_Class'] = 'Bonus_Pay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04eb6e6-90a6-4855-ba81-e8850bd99a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "got_bonus_df = bonus_df[bonus_df['Full_Total_Bonus'] > 0]\n",
    "got_bonus_df['Full_Total_Bonus'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69ea0a-9267-4c1b-85d2-afbad5ec8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of unique Personnel_No\n",
    "unique_personnel_no_count = bonus_df['Personnel_No'].nunique()\n",
    "\n",
    "# Filter for personnel with Full_Total_Bonus > 0\n",
    "bonus_df_filtered = bonus_df[bonus_df['Full_Total_Bonus'] > 0]\n",
    "\n",
    "# Count and calculate percentage of personnel with bonus > 0\n",
    "personnel_with_bonus_count = bonus_df_filtered['Personnel_No'].nunique()\n",
    "personnel_with_bonus_percentage = (personnel_with_bonus_count / unique_personnel_no_count) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Number of unique Personnel_No: {unique_personnel_no_count}\")\n",
    "print(f\"Count of personnel with Full_Total_Bonus > 0: {personnel_with_bonus_count}\")\n",
    "print(f\"Percentage of personnel with Full_Total_Bonus > 0: {personnel_with_bonus_percentage:.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7d094-93b1-4d7d-85a0-096f7444b435",
   "metadata": {},
   "source": [
    "##  Save the data to Excel before it's filtered to the payment period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3498e65-90fe-41b0-b0e5-bf6db2810d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.worksheet.table import Table, TableStyleInfo\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "def create_excel_with_tables(dfs, sheet_names, file_path):\n",
    "    \"\"\"\n",
    "    Create an Excel file with multiple sheets, each containing a DataFrame formatted as a table.\n",
    "\n",
    "    :param dfs: List of DataFrames.\n",
    "    :param sheet_names: List of sheet names corresponding to the DataFrames.\n",
    "    :param file_path: Path to save the Excel file.\n",
    "    \"\"\"\n",
    "    if len(dfs) != len(sheet_names):\n",
    "        raise ValueError(\"The number of DataFrames and sheet names must be the same\")\n",
    "\n",
    "    # Create a new workbook\n",
    "    wb = Workbook()\n",
    "\n",
    "    for df, sheet_name in zip(dfs, sheet_names):\n",
    "        # Create a new sheet\n",
    "        ws = wb.create_sheet(title=sheet_name)\n",
    "\n",
    "        # Adding DataFrame to sheet\n",
    "        for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), 1):\n",
    "            for c_idx, value in enumerate(row, 1):\n",
    "                ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "        # Create a table\n",
    "        table = Table(displayName=sheet_name, ref=ws.dimensions)\n",
    "\n",
    "        # Add a predefined style\n",
    "        style = TableStyleInfo(name=\"TableStyleLight8\", showFirstColumn=False,\n",
    "                               showLastColumn=False, showRowStripes=True, showColumnStripes=True)\n",
    "        table.tableStyleInfo = style\n",
    "        ws.add_table(table)\n",
    "\n",
    "    # Remove the default sheet created by openpyxl\n",
    "    if 'Sheet' in wb.sheetnames:\n",
    "        wb.remove(wb['Sheet'])\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7824eb7-6c98-4e6b-bc13-598281a1d23f",
   "metadata": {},
   "source": [
    "## Now the bonuses have been prorated we need to filter by date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb833e-c74c-4166-92d7-23ded1f8b6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_wage_df = wage_df[(wage_df['Payment_Date'] >= START_DATE) & (wage_df['Payment_Date'] <= END_DATE)]\n",
    "\n",
    "# Extract the unique Personnel_No values from stafflist_df\n",
    "valid_personnel_nos = stafflist_df['Personnel_No'].unique()\n",
    "\n",
    "# Drop anyone who isn't an employee\n",
    "filtered_wage_df = filtered_wage_df[filtered_wage_df['Personnel_No'].isin(valid_personnel_nos)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ce59e3-0b9f-4dc2-9a11-7dfacb8cbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_df['Wage_Class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe04081-5f53-4dfd-872b-426d1fa3687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Pivot the DataFrame and sum both 'Amount' and 'Number' columns\n",
    "pivot_df = filtered_wage_df.pivot_table(index='Personnel_No', \n",
    "                               columns='Wage_Class', \n",
    "                               values=['Amount', 'Number'], \n",
    "                               aggfunc={'Amount': 'sum', 'Number': 'sum'}, \n",
    "                               fill_value=0)\n",
    "\n",
    "# Reset the index to make 'Personnel_No' a column\n",
    "pivot_df.reset_index(inplace=True)\n",
    "\n",
    "pivot_df.columns = ['_'.join(col).strip() for col in pivot_df.columns.values]\n",
    "\n",
    "# Rename 'Personnel_No_'\n",
    "pivot_df.rename(columns={'Personnel_No_': 'Personnel_No'}, inplace=True)\n",
    "\n",
    "# Change 'Amount_' to 'Total_'\n",
    "pivot_df.rename(columns={col: col.replace('Amount_', 'Total_') for col in pivot_df.columns if col.startswith('Amount_')}, inplace=True)\n",
    "\n",
    "# Rename 'Number_Ordinary_Casual'\n",
    "pivot_df.rename(columns={'Number_Ordinary_Casual': 'Total_Casual_Hours'}, inplace=True)\n",
    "\n",
    "# Drop unwanted columns\n",
    "pivot_df.drop(columns=['Number_Bonus_Pay', 'Number_Ordinary_Pay', 'Number_Salary_Sacrifice'], inplace=True)\n",
    "\n",
    "# Define the date range for the 12-week period prior to March 31, 2023\n",
    "end_date = pd.to_datetime('2023-03-31')\n",
    "start_date = end_date - pd.Timedelta(weeks=12)\n",
    "\n",
    "# Filter wage_df for the 12-week period\n",
    "filtered_wage_df_12weeks = wage_df[(wage_df['Payment_Date'] >= start_date) & (wage_df['Payment_Date'] <= end_date)]\n",
    "\n",
    "# Identify Personnel_No with 'Ordinary_Casual' payments (where Wage_Class is \"Ordinary_Casual\" and the amount is > 0) and 'Number' > 0 in March 2023\n",
    "march_personnel = wage_df[(wage_df['Payment_Date'] >= '2023-03-01') & \n",
    "                          (wage_df['Payment_Date'] <= '2023-03-31') & \n",
    "                          (wage_df['Wage_Class'] == 'Ordinary_Casual') & \n",
    "                          (wage_df['Amount'] > 0) & \n",
    "                          (wage_df['Number'] > 0)]['Personnel_No'].unique()\n",
    "\n",
    "# Filter the 12-week DataFrame for these Personnel_No\n",
    "filtered_wage_df_12weeks = filtered_wage_df_12weeks[filtered_wage_df_12weeks['Personnel_No'].isin(march_personnel)]\n",
    "\n",
    "# Sum and average Total_Ordinary_Casual and Total_Casual_Hours over the 12 weeks\n",
    "average_wage = filtered_wage_df_12weeks[filtered_wage_df_12weeks['Wage_Class'] == 'Ordinary_Casual'].groupby('Personnel_No').agg({\n",
    "    'Amount': 'sum',\n",
    "    'Number': 'sum' \n",
    "}).reset_index()\n",
    "average_wage['Average_Casual_Pay'] = average_wage['Amount'] / 3  # Dividing by 3 to average over 12 weeks\n",
    "average_wage['Average_Casual_Hours'] = average_wage['Number'] / 3\n",
    "\n",
    "# Merge with wage_df_merged to overwrite Total_Casual_Pay and Total_Casual_Hours\n",
    "wage_df_merged = filtered_wage_df.merge(average_wage[['Personnel_No', 'Average_Casual_Pay', 'Average_Casual_Hours']], on='Personnel_No', how='left')\n",
    "\n",
    "# Replace values in wage_df_merged\n",
    "wage_df_merged.loc[wage_df_merged['Personnel_No'].isin(march_personnel), 'Total_Ordinary_Casual'] = wage_df_merged['Average_Casual_Pay']\n",
    "wage_df_merged.loc[wage_df_merged['Personnel_No'].isin(march_personnel), 'Total_Casual_Hours'] = wage_df_merged['Average_Casual_Hours']\n",
    "\n",
    "# Drop the temporary average columns\n",
    "wage_df_merged.drop(columns=['Average_Casual_Pay', 'Average_Casual_Hours'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa5b443-e9bc-4130-a083-73d53ee0c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinary_casual_pay = wage_df_merged['Total_Ordinary_Casual'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e505e-99dd-40d3-ae4d-4fcb0732443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinary_casual_hours = wage_df_merged['Total_Casual_Hours'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9bc445-6751-43eb-a4a5-3d66fd66caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wagepivot_df = filtered_wage_df.merge(pivot_df, on='Personnel_No', how='left')\n",
    "wage_nodupe_df = wagepivot_df.drop_duplicates(subset='NINO')\n",
    "wage_nodupe_df.drop_duplicates(subset='Personnel_No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73137a95-4274-4af4-9294-a096f913be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## BONUS CALCS HERE\n",
    "wage_nodupe_df['Prorated_Bonus_Total'].describe()\n",
    "\n",
    "\n",
    "# Pivot for Total_Bonus_Pay and Total_Bonus_Count\n",
    "bonus_pay_df = wage_nodupe_df[(wage_nodupe_df['Wage_Class'] == 'Bonus_Pay') & \n",
    "                       (wage_nodupe_df['Payment_Date'] >= '2022-04-01') & \n",
    "                       (wage_nodupe_df['Payment_Date'] <= '2023-03-31')]\n",
    "pivot_bonus_pay = bonus_pay_df.pivot_table(index='Personnel_No', \n",
    "                                           values='Amount', \n",
    "                                           aggfunc={'Amount': 'sum', 'Personnel_No': 'count'}, \n",
    "                                           fill_value=0)\n",
    "pivot_bonus_pay.rename(columns={'Amount': 'Full_Total_Bonus', 'Personnel_No': 'Total_Bonus_Count'}, inplace=True)\n",
    "\n",
    "# Merge pivoted data back into wage_df\n",
    "wage_nodupe_df = wage_nodupe_df.merge(pivot_bonus_pay, on='Personnel_No', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448689f8-3b9d-430b-aa08-b0f4af1021de",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Check against National Minimum Wage or National Living Wage. \n",
    "Uncomment the last line below to apply a NMW floor to hourly rate</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5465f5b-a1cf-4afa-bc91-02dd3cec445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Calculate NMW per individual\n",
    "wage_nodupe_df['Date_of_Birth'] = pd.to_datetime(wage_nodupe_df['Date_of_Birth'])\n",
    "\n",
    "# Define NMW rates for March 2023\n",
    "nmw_rates = {\n",
    "    '23 and over': 9.50,\n",
    "    '21 to 22': 9.18,\n",
    "    '18 to 20': 6.83\n",
    "}\n",
    "\n",
    "# Reference date: March 31, 2023\n",
    "reference_date = datetime(2023, 3, 31)\n",
    "\n",
    "# Calculate age on reference date\n",
    "wage_nodupe_df['Age_on_Reference'] = wage_nodupe_df['Date_of_Birth'].apply(lambda dob: reference_date.year - dob.year - ((reference_date.month, reference_date.day) < (dob.month, dob.day)))\n",
    "\n",
    "# Assign NMW based on age\n",
    "wage_nodupe_df['NMW'] = wage_nodupe_df['Age_on_Reference'].apply(\n",
    "    lambda age: nmw_rates['23 and over'] if age >= 23 else (\n",
    "        nmw_rates['21 to 22'] if age >= 21 else (\n",
    "            nmw_rates['18 to 20'] if age >= 18 else 0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "#wage_nodupe_df[['Age_on_Reference', 'NMW']]\n",
    "\n",
    "#wage_nodupe_df['HourlyExcSS'] = wage_nodupe_df['HourlyExcSS'].apply(lambda x: max(x, NMW))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8a5b5-b76c-4753-8257-2abedaa26230",
   "metadata": {},
   "source": [
    "##  Calculate Hourly rate inclusive and exclusive of salary sacrifice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26daa530-e8a5-40e1-bcbb-4852bf45b342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that FTE is a numeric and not a string or object type\n",
    "wage_nodupe_df['FTE'] = pd.to_numeric(wage_nodupe_df['FTE'], errors='coerce')\n",
    "\n",
    "# Calculate WeeklyHours as a percentage of 37 based on FTE for ordinary staff\n",
    "wage_nodupe_df.loc[wage_nodupe_df['FTE'] > 0, 'WeeklyHours'] = (37 * wage_nodupe_df['FTE']) / 100\n",
    "\n",
    "# Convert 'Total_Ordinary_Casual' and 'Number' to numeric, handling non-numeric values\n",
    "wage_nodupe_df['Total_Ordinary_Casual'] = pd.to_numeric(wage_nodupe_df['Total_Ordinary_Casual'], errors='coerce')\n",
    "wage_nodupe_df['Number'] = pd.to_numeric(wage_nodupe_df['Number'], errors='coerce')\n",
    "\n",
    "# Calculate HourlyExcSS based on the type of staff (ordinary or casual)\n",
    "# For ordinary staff (FTE > 0), use the existing formula\n",
    "# For casual staff (FTE == 0 or NaN), use Total_Ordinary_Casual / Total_Casual_Hours\n",
    "wage_nodupe_df['HourlyExcSS'] = np.where(\n",
    "    wage_nodupe_df['FTE'] > 0,\n",
    "    (wage_nodupe_df['Total_Ordinary_Pay'] * 7) / (30.44 * wage_nodupe_df['WeeklyHours']),\n",
    "    wage_nodupe_df['Total_Ordinary_Casual'] / wage_nodupe_df['Total_Casual_Hours']\n",
    ")\n",
    "\n",
    "# Handle division by zero or NaN values if 'Number' column can have 0 or NaN values\n",
    "wage_nodupe_df['HourlyExcSS'] = wage_nodupe_df['HourlyExcSS'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Calculate HourlyIncSS based on the type of staff (ordinary or casual)\n",
    "wage_nodupe_df['HourlyIncSS'] = np.where(\n",
    "    wage_nodupe_df['FTE'] > 0,\n",
    "    ((wage_nodupe_df['Total_Ordinary_Pay'] + wage_nodupe_df['Total_Salary_Sacrifice']) * 7) / (30.44 * wage_nodupe_df['WeeklyHours']),\n",
    "    (wage_nodupe_df['Total_Ordinary_Casual'] + wage_nodupe_df['Total_Salary_Sacrifice']) / wage_nodupe_df['Total_Casual_Hours']\n",
    ") # add the total as SS deducted from gross pay is negative\n",
    "\n",
    "# Handle division by zero or NaN values if 'HourlyIncSS' column can have 0 or NaN values\n",
    "wage_nodupe_df['HourlyIncSS'] = wage_nodupe_df['HourlyIncSS'].replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45899895-21ea-4220-b984-9b8c84b0e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['Employee_Classification'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51ac753-63ab-4a6d-a567-6c3372c8b6ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Do check on minimum hourly rates and \n",
    "any individuals who are not full-pay relevant, using inc salary sacrifice</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5fe27d-6dce-4d0b-a95a-535eb719084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['NMW_Check'] = wage_nodupe_df['HourlyIncSS'] < wage_nodupe_df['NMW']\n",
    "nmw_true_rows = wage_nodupe_df[wage_nodupe_df['NMW_Check'] == True]\n",
    "# Set 'Employee_Classification' to 'Relevant' where 'NMW_Check' is True\n",
    "wage_nodupe_df.loc[wage_nodupe_df['NMW_Check'] == True, 'Employee_Classification'] = 'Relevant'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53941e19-fac7-4433-94ac-ad5a5acfdde2",
   "metadata": {},
   "source": [
    "## Add domicile, ethnicitye2 and ethnicitye5 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb626992-ad9d-407b-8e31-3f4895904c67",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">create UKNonUK (domicile)</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ee7632-e0f5-44f0-a414-10a5e31af835",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['Nationality'].value_counts()\n",
    "\n",
    "def map_domicile(column):\n",
    "    # Lowercase the input for case-insensitive comparison\n",
    "    lowercased = column.str.lower()\n",
    "\n",
    "    # Define the mapping conditions\n",
    "    conditions = [\n",
    "        lowercased.str.contains('refused|not provided|prefer not to say|not available|unknown'),\n",
    "        lowercased.str.contains('british'),\n",
    "    ]\n",
    "\n",
    "    # Define the mapping values\n",
    "    values = ['Unknown','UK']\n",
    "\n",
    "    # Apply the mapping\n",
    "    return pd.Series(np.select(conditions, values, default='NonUK'), index=column.index)\n",
    "\n",
    "wage_nodupe_df['UKNonUK'] = map_domicile(wage_nodupe_df['Nationality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46039976-a32e-439e-81b6-bae06286d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['UKNonUK'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc26f62e-a597-42a7-896f-23720a55903b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">create 2 factor ethnicity</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62a41a8-e726-4964-b952-22a3d725a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ethnicitye2(column):\n",
    "    # Lowercase the input for case-insensitive comparison\n",
    "    lowercased = column.str.lower()\n",
    "\n",
    "    # Define the mapping conditions\n",
    "    conditions = [\n",
    "        lowercased.str.contains('refused|not provided|prefer not to say|not available|unknown'),\n",
    "        lowercased.str.contains('asian|chinese') & ~lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('black') & ~lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('white') & ~lowercased.str.contains('mixed|gypsy|traveller'),\n",
    "        lowercased.str.contains('gypsy|traveller'),\n",
    "        lowercased.str.contains('arab'),\n",
    "        lowercased.str.contains('other') & ~lowercased.str.contains('white|black|mixed|asian')\n",
    "    ]\n",
    "\n",
    "    # Define the mapping values\n",
    "    values = ['Unknown', 'BAME', 'BAME', 'BAME', 'White', 'White', 'BAME', 'BAME']\n",
    "\n",
    "    # Apply the mapping\n",
    "    return pd.Series(np.select(conditions, values, default='Other'), index=column.index)\n",
    "\n",
    "wage_nodupe_df['EthnicityE2'] = map_ethnicitye2(wage_nodupe_df['Ethnicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1746f7ca-220d-48cd-9923-efdf0e20bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['EthnicityE2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eed607-9c1a-4793-98d7-98765716920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ethnicity_values = ['Unknown']\n",
    "ethnicity_df = wage_nodupe_df.loc[wage_nodupe_df['EthnicityE2'].isin(ethnicity_values), ['Ethnicity', 'EthnicityE2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a51cd7-b76f-470f-bb62-5dcb3a0dca50",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">create 5 factor ethnicity</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10178df1-01af-442b-8220-c92ee21f65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_ethnicitye5(column):\n",
    "    # Lowercase the input for case-insensitive comparison\n",
    "    lowercased = column.str.lower()\n",
    "\n",
    "    # Define the mapping conditions\n",
    "    conditions = [\n",
    "        lowercased.str.contains('refused|not provided|prefer not to say|not available|unknown'),\n",
    "        lowercased.str.contains('asian|chinese') & ~lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('black') & ~lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('mixed'),\n",
    "        lowercased.str.contains('white') & ~lowercased.str.contains('mixed|gypsy|traveller'),\n",
    "        lowercased.str.contains('gypsy|traveller'),\n",
    "        lowercased.str.contains('arab'),\n",
    "        lowercased.str.contains('other') & ~lowercased.str.contains('white|black|mixed|asian')\n",
    "    ]\n",
    "\n",
    "    # Define the mapping values\n",
    "    values = ['Unknown', 'Asian', 'Black', 'Mixed', 'White', 'White', 'Other', 'Other']\n",
    "\n",
    "    # Apply the mapping\n",
    "    return pd.Series(np.select(conditions, values, default='Other'), index=column.index)\n",
    "\n",
    "wage_nodupe_df['EthnicityE5'] = map_ethnicitye5(wage_nodupe_df['Ethnicity'])\n",
    "# Define the custom order\n",
    "custom_order = ['Asian', 'Black', 'Mixed', 'Other', 'White', 'Unknown']\n",
    "\n",
    "# Convert 'EthnicityE5' to a categorical column with custom order\n",
    "wage_nodupe_df['EthnicityE5'] = pd.Categorical(wage_nodupe_df['EthnicityE5'], categories=custom_order, ordered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e44db9-d7f0-4461-aed3-fd7d67d6193f",
   "metadata": {},
   "source": [
    "## Save the data to Excel now its been filtered to the payment period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3759857-589c-409a-a52a-07e28c511571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "wage_nodupe_relandfprel_df = wage_nodupe_df\n",
    "# Full-pay relevant only from here\n",
    "wage_nodupe_df = wage_nodupe_df[wage_nodupe_df['Employee_Classification'] != 'Relevant']\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Title', 'Forename', 'Preferred_Forename', 'Surname', 'Contract', \n",
    "    'Date_First_Hired', 'Latest_Hire_Date', \n",
    "    'Projected_End_Date', 'Current_Assignment', \n",
    "    'Position', 'Position_ID', 'Job', 'Responsibility', 'Responsibility_Start_Date', \n",
    "    'Responsibility_End_Date', 'Line_Manager_(Current)', 'Organisation_Level_0', \n",
    "    'Organisation_Section', 'Doctorally_Qualified', 'HEA', 'HEA_Category', 'CRB_Status', \n",
    "    'Working_Time', 'Working_Hours_per_Week', 'Grade_Point', \n",
    "    'Annual_Base_Salary', 'FTE_Annual_Base_Salary', 'Contracted_Salary', 'NI_Number', 'Pension_Scheme', 'Master_CC', \n",
    "    'Dept_-_Master_CC', 'HESA_ACC', 'REF_Eligible', 'REF\\'able', 'Site', \n",
    "    'Onsite_Location', 'User_ID', 'Email', 'DLA_Summary', 'DLA_Category', 'SOC_Code', \n",
    "    'Company', 'Status', 'Organisation_Sub_Group', 'Group_5_Desc', 'Surname_Name', \n",
    "    'Name', 'NINO', 'Comp_Code', 'Comp_Name', 'Pers_Area', 'Pers_Area_Text', \n",
    "    'Payroll_Area', 'Payroll_Text', 'Period_Params', 'Period_Params_Name', \n",
    "    'For_Period', 'Payment_Date', 'Grouping', 'Wage_Type', 'Wage_Class', 'Wage_Text', \n",
    "    'Daily_Hours', 'Weekly_Days', 'Work_Hours_Period', 'FTE', 'Pay_Scale', \n",
    "    'Pay_Scale_Text', 'Pay_Scale_Area', 'Pay_Scale_Area_Text', 'Grade', 'Point', \n",
    "    'Number', 'Amount', 'Currency', 'Staff_Id_y'\n",
    "]\n",
    "\n",
    "wage_short = wage_nodupe_relandfprel_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Select only the required columns from got_bonus_df along with the column to join on\n",
    "got_bonus_df_subset = got_bonus_df[['Personnel_No', 'Full_Total_Bonus', 'Total_Bonus_Count']]\n",
    "\n",
    "# Perform the left merge\n",
    "ws_merged_df = pd.merge(wage_short, got_bonus_df_subset, how='left', on='Personnel_No')\n",
    "\n",
    "columns_to_drop = ['Full_Total_Bonus_x', 'Total_Bonus_Count_x']\n",
    "ws_merged_df.drop(columns_to_drop, axis=1, errors='ignore', inplace=True)\n",
    "\n",
    "# Rename columns only if they exist in the DataFrame\n",
    "columns_to_rename = {'Full_Total_Bonus_y': 'Full_Total_Bonus', 'Total_Bonus_Count_y': 'Total_Bonus_Count'}\n",
    "ws_merged_df.rename(columns=columns_to_rename, errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c922cf-127c-46ea-b7e1-07d61892ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and filename\n",
    "directory = r'directory'\n",
    "filename = 'filename'\n",
    "# Create the full path to the CSV file\n",
    "file_path = os.path.join(directory, filename)\n",
    "ws_merged_df['Include Flag'] = ws_merged_df['Employee_Classification'].apply(lambda x: 'Yes' if x == 'Full-pay relevant' else '')\n",
    "ws_merged_df.rename(columns={'Staff_Id_x': 'Staff_Id'}, inplace=True)\n",
    "# Save the DataFrame to the CSV file\n",
    "#ws_merged_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8faf0f-eeb8-473b-9962-c056cff5e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptive statistics for 'HourlyExcSS'. Expected values mean and median 20 - 25 with a max of around 130\n",
    "stats_hourly_exc_ss = wage_nodupe_df['HourlyExcSS'].describe()\n",
    "stats_hourly_inc_ss = wage_nodupe_df['HourlyIncSS'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93add1-e0c6-4d1b-bdc9-f8175fa604f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wage_nodupe_df['HourlyIncSS'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d5e0e4-aa8b-409c-a5d1-fbe07f58d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check for the highest hourly rate - if this returns an unexpected row then further checks needed\n",
    "idx_max = wage_nodupe_df['HourlyExcSS'].idxmax()\n",
    "max_HourlyExcSS_row = wage_nodupe_df.loc[idx_max]\n",
    "specified_columns_df = max_HourlyExcSS_row[['Title', 'Forename', 'Surname', 'HourlyExcSS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35faf4c3-7818-4386-8462-965a5d5df382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Gender' and calculate median and mean of 'HourlyExcSS'\n",
    "gender_stats = wage_nodupe_df.groupby('Gender')['HourlyExcSS'].agg(['median', 'mean']).reset_index()\n",
    "\n",
    "# Calculate % difference of Female compared to Male for median and mean\n",
    "female_median = gender_stats.loc[gender_stats['Gender'] == 'Female', 'median'].values[0]\n",
    "male_median = gender_stats.loc[gender_stats['Gender'] == 'Male', 'median'].values[0]\n",
    "female_mean = gender_stats.loc[gender_stats['Gender'] == 'Female', 'mean'].values[0]\n",
    "male_mean = gender_stats.loc[gender_stats['Gender'] == 'Male', 'mean'].values[0]\n",
    "\n",
    "# Use absolute value to express the difference as a positive number\n",
    "percent_diff_median = abs((female_median - male_median) / male_median) * 100\n",
    "percent_diff_mean = abs((female_mean - male_mean) / male_mean) * 100\n",
    "\n",
    "# Add row for % difference\n",
    "percent_diff_row = pd.DataFrame({\n",
    "    'Gender': ['% difference'],\n",
    "    'median': [percent_diff_median],\n",
    "    'mean': [percent_diff_mean]\n",
    "})\n",
    "\n",
    "# Combine tables\n",
    "result_table = pd.concat([gender_stats, percent_diff_row], ignore_index=True)\n",
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf74dd15-d65e-42b1-b64d-bdb461da3fdf",
   "metadata": {},
   "source": [
    "## All calculations are now done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2396efdb-4bf9-41c7-adb7-380bf6e33c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = wage_nodupe_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed337292-e602-45ee-9ae7-9837ec6a6ac3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">Use ydata profiling for EDA now wage df is concatenated - uncomment to use</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1093f7-70dc-4ce0-8540-ecf38ae605e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#profile = ProfileReport(wage_nodupe_df, title=\"Profiling Report\")\n",
    "#profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14dec18-5cc2-429e-aed4-08527263d179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a boxplot of the female and male values for HourlyExcSS using seaborn with £ on x-axis\n",
    "plt.figure(figsize=(16, 9))  # Adjusting figure size to 16:9 ratio\n",
    "ax = sns.boxplot(x='HourlyExcSS', y='Gender', data=df_all, palette=\"Set3\", orient='h')\n",
    "plt.title('Boxplot of HourlyExcSS by Gender')\n",
    "ax.set(xlabel='HourlyExcSS (£)', ylabel='Gender')  # Adding £ symbol to the x-axis label\n",
    "# Format x-axis ticks with £ symbol\n",
    "fmt = '£{x:,.0f}'\n",
    "tick = plt.FuncFormatter(lambda x, _: fmt.format(x=x))\n",
    "ax.xaxis.set_major_formatter(tick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b0ca4-77b4-452b-bb91-4b33adbe4058",
   "metadata": {},
   "source": [
    "## FUNCTION TO CREATE TABLES SHOWING MEAN AND MEDIAN DIFFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006243f-9767-4b06-acbc-33c431fb3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats_compared_to_group(df, characteristic, metric, comparator):\n",
    "    # Group by characteristic and calculate median, mean, count for the metric\n",
    "    stats = df.groupby(characteristic)[metric].agg(['median', 'mean', 'count']).reset_index()\n",
    "\n",
    "    # Calculate the total count to use for percentage calculation\n",
    "    total_count = stats['count'].sum()\n",
    "\n",
    "    # Calculate % count for each group and add it as a new column to stats\n",
    "    stats['% count'] = (stats['count'] / total_count) * 100\n",
    "\n",
    "    # Calculate the values for the comparator group\n",
    "    comparator_stats = stats[stats[characteristic] == comparator]\n",
    "\n",
    "    # Initialize a DataFrame to hold the percent differences\n",
    "    percent_diffs = pd.DataFrame(columns=[characteristic, 'median', 'mean', 'count', '% count'])\n",
    "\n",
    "    for _, row in stats.iterrows():\n",
    "        if row[characteristic] != comparator:\n",
    "            # Calculate % difference for median and mean compared to the comparator (reverse the order of subtraction)\n",
    "            percent_diff_median = ((comparator_stats['median'].values[0] - row['median']) / comparator_stats['median'].values[0]) * 100\n",
    "            percent_diff_mean = ((comparator_stats['mean'].values[0] - row['mean']) / comparator_stats['mean'].values[0]) * 100\n",
    "            \n",
    "            # Calculate count difference and % count difference compared to the comparator\n",
    "            diff_count = row['count'] - comparator_stats['count'].values[0]\n",
    "            percent_diff_count = row['% count'] - comparator_stats['% count'].values[0]\n",
    "        \n",
    "            # Create a new row as a DataFrame to add to percent_diffs\n",
    "            new_row = pd.DataFrame({\n",
    "                characteristic: [f\"difference from {comparator} ({row[characteristic]})\"],\n",
    "                'median': [percent_diff_median],\n",
    "                'mean': [percent_diff_mean],\n",
    "                'count': [diff_count],\n",
    "                '% count': [percent_diff_count]\n",
    "            })\n",
    "        \n",
    "            # Drop empty or all-NA columns from new_row before concatenating\n",
    "            new_row = new_row.dropna(axis='columns', how='all')\n",
    "        \n",
    "            # Use pd.concat with the cleaned new_row\n",
    "            percent_diffs = pd.concat([percent_diffs, new_row], ignore_index=True)\n",
    "    \n",
    "    # Combine tables\n",
    "    result_table = pd.concat([stats, percent_diffs], ignore_index=True)\n",
    "\n",
    "    # Format the result_table\n",
    "    result_table['mean'] = result_table.apply(lambda x: f\"£{x['mean']:.2f}\" if not x[characteristic].startswith('difference from') else f\"{x['mean']:.1f}%\", axis=1)\n",
    "    result_table['median'] = result_table.apply(lambda x: f\"£{x['median']:.2f}\" if not x[characteristic].startswith('difference from') else f\"{x['median']:.1f}%\", axis=1)\n",
    "    result_table['count'] = result_table['count'].apply(lambda x: f\"{x:.0f}\")\n",
    "    result_table['% count'] = result_table['% count'].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954d703f-5789-4c67-b99b-d3483e0a7dd4",
   "metadata": {},
   "source": [
    "## Mean and median salary sacrifice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c109073-a79d-45fe-b492-9d72c00f2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where Total_Salary_Sacrifice > 0\n",
    "salary_sacrifice_df = df_all[df_all['Total_Salary_Sacrifice'] > 0]\n",
    "\n",
    "# Group by 'Gender' and calculate the required statistics\n",
    "gender_stats = salary_sacrifice_df.groupby('Gender').agg(\n",
    "    Count=('Total_Salary_Sacrifice', 'size'),\n",
    "    Mean=('Total_Salary_Sacrifice', 'mean'),\n",
    "    Median=('Total_Salary_Sacrifice', 'median')\n",
    ")\n",
    "\n",
    "# Calculate the percentage of each gender\n",
    "gender_stats['Percentage'] = (gender_stats['Count'] / gender_stats['Count'].sum()) * 100\n",
    "\n",
    "# Format the 'Percentage', 'Mean', and 'Median' columns\n",
    "gender_stats['Percentage'] = gender_stats['Percentage'].map('{:.1f}%'.format)\n",
    "gender_stats['Mean'] = gender_stats['Mean'].map('£{:.2f}'.format)\n",
    "gender_stats['Median'] = gender_stats['Median'].map('£{:.2f}'.format)\n",
    "salary_sacrifice_table = gender_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f15e0ff-c276-4c33-8843-75081da8e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_eth_incss = calculate_stats_compared_to_group(df_all, 'EthnicityE5', 'HourlyIncSS', 'White')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ae55d-5a16-4a37-b1e7-71fbdfe30f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Create a boxplot of the female and male values for HourlyExcSS using seaborn with £ on x-axis\n",
    "plt.figure(figsize=(16, 9))  # Adjusting figure size to 16:9 ratio\n",
    "ax = sns.boxplot(x='HourlyExcSS', y='EthnicityE5', data=wage_nodupe_df, palette=\"Set3\", orient='h')\n",
    "plt.title('Boxplot of HourlyExcSS by EthnicityE5')\n",
    "ax.set(xlabel='HourlyExcSS (£)', ylabel='EthnicityE5')  # Adding £ symbol to the x-axis label\n",
    "# Format x-axis ticks with £ symbol\n",
    "fmt = '£{x:,.0f}'\n",
    "tick = plt.FuncFormatter(lambda x, _: fmt.format(x=x))\n",
    "ax.xaxis.set_major_formatter(tick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b2825e-d433-4c7e-a1f2-155943bfff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_all_gen = calculate_stats_compared_to_group(got_bonus_df, 'Gender', 'Full_Total_Bonus', 'Male')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fa1e13-a729-44e6-adb5-782870e1f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_stats = got_bonus_df['Full_Total_Bonus'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b291e47-5155-4081-a578-2331fa13dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nusl = df_all[df_all['Payroll_Area_SL'] == 'M2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d7095e-a55f-46de-ba42-6f368cd0f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gen_excss = calculate_stats_compared_to_group(df_all, 'Gender', 'HourlyExcSS', 'Male')\n",
    "all_gen_incss = calculate_stats_compared_to_group(df_all, 'Gender', 'HourlyIncSS', 'Male')\n",
    "nusl_gen_excss = calculate_stats_compared_to_group(df_nusl, 'Gender', 'HourlyExcSS', 'Male')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41047f23-bd7e-44ef-b131-ed5874fc0811",
   "metadata": {},
   "source": [
    "## Calculate Quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9570432-bcc5-4913-a540-4ae207e68e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_add_quartiles(df, filter_condition=None):\n",
    "    # Apply the filtering condition to the DataFrame if provided\n",
    "    if filter_condition is not None:\n",
    "        filtered_df = df[filter_condition]\n",
    "    else:\n",
    "        filtered_df = df.copy()  # Make a copy of the original DataFrame if no filter condition provided\n",
    "    \n",
    "    # Define quartile labels\n",
    "    labels = [\"Lower\", \"Lower Mid\", \"Upper Mid\", \"Upper\"]\n",
    "    \n",
    "    # Calculate quartiles using the specified labels and method\n",
    "    filtered_df['Quartile'] = pd.qcut(filtered_df['HourlyIncSS'].rank(method='first'), q=4, labels=labels)\n",
    "    \n",
    "    # Create a dictionary mapping Personnel_No to Quartile\n",
    "    quartile_mapping = dict(filtered_df[['Personnel_No', 'Quartile']].values)\n",
    "    \n",
    "    # Add the 'Quartile' column to the original DataFrame using map\n",
    "    df['Quartile'] = df['Personnel_No'].map(quartile_mapping)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc26e2-7c7a-4b4f-88bf-4261992347d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all_q = calculate_and_add_quartiles(df_all)\n",
    "#all_ps_q = calculate_and_add_quartiles(df_all, (df_all['Employee_Type'] == 'Professional Support'))\n",
    "# UNN ['Payroll_Area_SL'] == 'M1']\n",
    "# NUSL ['Payroll_Area_SL'] == 'M2']\n",
    "# ACA ['Employee_Type'] == 'Academic']\n",
    "# PS ['Employee_Type'] == 'Professional Support']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1198b7-0e0c-42f1-83d2-2b2017f879ec",
   "metadata": {},
   "source": [
    "## Summarise quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c94c84-a288-42f3-868b-698fb925bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quartile_summary(df, characteristic_column, quartile_column):\n",
    "    # Define custom order for Quartile\n",
    "    custom_order = [\"Lower\", \"Lower Mid\", \"Upper Mid\", \"Upper\"]\n",
    "    \n",
    "    # Group the DataFrame by the quartile and characteristic columns\n",
    "    grouped = df.groupby([quartile_column, characteristic_column])['Personnel_No'].count().reset_index()\n",
    "    \n",
    "    # Rename the 'Personnel_No' column to 'Count'\n",
    "    grouped.rename(columns={'Personnel_No': 'Count'}, inplace=True)\n",
    "    \n",
    "    # Calculate the total count of Personnel_No for each characteristic\n",
    "    total_counts = grouped.groupby(characteristic_column)['Count'].sum()\n",
    "    \n",
    "    # Calculate the percentage of each quartile count relative to the total count for each characteristic\n",
    "    grouped['Percentage'] = grouped.apply(lambda row: (row['Count'] / total_counts[row[characteristic_column]]) * 100, axis=1)\n",
    "    \n",
    "    # Format the 'Percentage' column to one decimal place\n",
    "    grouped['Percentage'] = grouped['Percentage'].round(1)\n",
    "    \n",
    "    # Reorder the columns as Quartile, Characteristic, Count, Percentage\n",
    "    grouped = grouped[['Quartile', characteristic_column, 'Count', 'Percentage']]\n",
    "    \n",
    "    # Set custom order for Quartile column\n",
    "    grouped['Quartile'] = pd.Categorical(grouped['Quartile'], categories=custom_order, ordered=True)\n",
    "    \n",
    "    # Sort the DataFrame based on the custom order of Quartile\n",
    "    grouped = grouped.sort_values(by='Quartile')\n",
    "    \n",
    "    return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9665ca37-8b7c-49bb-ac3a-2967cd05ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_quartiles = quartile_summary(all_all_q, 'Gender', 'Quartile')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565323d-5b32-4f4b-9c01-4eb998013d42",
   "metadata": {},
   "source": [
    "## Export pay gap tables to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786afd42-55d2-4706-aac7-d581bd4ca19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "def create_excel_with_tables(dfs, sheet_names, file_path, fill_missing_values=False, fill_value=None, max_string_length=None):\n",
    "    # Create a new Excel workbook\n",
    "    wb = Workbook()\n",
    "    wb.remove(wb.active)  # Remove the default sheet\n",
    "\n",
    "    for df, sheet_name in zip(dfs, sheet_names):\n",
    "        # Ensure that the df is a DataFrame or a Series\n",
    "        if not isinstance(df, (pd.DataFrame, pd.Series)):\n",
    "            print(f\"Skipping {sheet_name}: Not a DataFrame or Series\")\n",
    "            continue\n",
    "\n",
    "        if isinstance(df, pd.Series):\n",
    "            # Convert Series to DataFrame\n",
    "            df = df.to_frame()\n",
    "\n",
    "        # Fill missing values if requested\n",
    "        if fill_missing_values:\n",
    "            fill_value_to_use = fill_value if fill_value is not None else 0\n",
    "            for column in df.select_dtypes(include=['category']).columns:\n",
    "                if fill_value_to_use not in df[column].cat.categories:\n",
    "                    df[column] = df[column].cat.add_categories([fill_value_to_use])\n",
    "            df.fillna(fill_value_to_use, inplace=True)\n",
    "\n",
    "        # Reset the index so that the index is included as a column\n",
    "        df = df.reset_index()\n",
    "\n",
    "        # Create a new sheet\n",
    "        ws = wb.create_sheet(title=sheet_name)\n",
    "\n",
    "        # Write headers\n",
    "        for col, header in enumerate(df.columns, 1):\n",
    "            ws.cell(row=1, column=col, value=header)\n",
    "\n",
    "        # Write data rows\n",
    "        for row, data_row in enumerate(df.itertuples(index=False), 2):\n",
    "            for col, value in enumerate(data_row, 1):\n",
    "                if max_string_length and isinstance(value, str):\n",
    "                    value = value[:max_string_length]\n",
    "                ws.cell(row=row, column=col, value=value)\n",
    "\n",
    "    # Save the workbook to the specified file path\n",
    "    wb.save(file_path)\n",
    "\n",
    "# Example usage with filling missing values and max_string_length\n",
    "dfs = [\n",
    "    wage_short,\n",
    "    stats_hourly_exc_ss,\n",
    "    stats_hourly_inc_ss,\n",
    "    salary_sacrifice_table,\n",
    "    bonus_all_gen,\n",
    "    bonus_stats,\n",
    "    all_gen_excss,\n",
    "    all_gen_incss,\n",
    "    gender_quartiles\n",
    "]\n",
    "sheet_names = [\"Main\", \"Casual_Pay\",\"Casual_Hours\", \"Salary Sacrifice\",\"Bonus Gender\",\n",
    "               \"Bonus Stats\",\"Gen Gap Ex\",\"Gen Gap Inc\",\n",
    "               \"Quartiles\"]\n",
    "# create_excel_with_tables(dfs, sheet_names, r\"filepath here\", fill_missing_values=True, fill_value=None, max_string_length=255)\n",
    "\n",
    "print(\"All processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
